# Извлечение информации с использованием больших языковых моделей (LLM)

Одна из основных проблем при использовании больших языковых моделей это практически неустранимые галлюцинации, возникающие при ответах на вопросы по загруженным документам. Задача "поговорить со своими документами" возникает очень часто, и как правило, она решается с помощью *промптинга* - вы загружаете вашу статью, договор или другой документ и пишете *промпт*

> «Ответь на вопрос по тексту: ...».

Этот способ работает, но у него есть существенные недостатки: размер документа ограничен 1-3 страницами, непредсказуемое возникновение галлюцинаций - неправильных ответов, выглядящих правдоподобно.

## Что такое RAG

**Retrieval-augmented generation** (далее RAG) дословно переводится как "Генерация с расширенным извлечением" и по сути она частично решает главную проблему языковых моделей - использование для ответов устаревших данных из "весов", что приводит к неправильным ответам. Например, такая проблема часто возникает, когда дать ответ нужно про актуальные события, даты или по некой внешней базе знаний(корпоративной), к которой вообще у модели не могло быть доступа. Основным преимуществом подхода является то, что модель не требуется переобучать на конкретную базу знаний (что может требовать немало времени и значительных вычислительных мощностей).

Подход RAG не новый и существует множество его реализаций - сейчас в основном типичным хорошим решением предлагается нарезать документ на *сниппеты* или *чанки* (кусочки текста, например разделы, абзацы), добавлять их в индекс сгенерировав для них векторные *эмбединги*, с помощью таких библиотек как LangChain, а затем с помощью векторной базы данных искать сниппеты/чанки, в которых могут находиться ответы и с помощью LLM (ChatGPT, GPT-4, локальной модели) генерировать ответ.

Подробнее про использование **RAG** для генерации ответов с опорой на конкретные знания можно почитать в статье [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf) - авторы статьи используют модель комбинируя её с RAG (для извлечения данных используется Wikipedia) т.о. подход сочетает в себе предварительно обученные параметрические и непараметрические механизмы памяти для улучшения доступа к фактическим знаниям и улучшения генерации языка в наукоемких задачах обработки естественного языка.

### Доступность больших языковых моделей

Обычный подход к улучшению качества больших языковых моделей сводится к увеличению датасета на котором обучается модель и <ins>увеличению количества параметров самой модели</ins> - т.о. обучение *лучшей* модели = обучение ***большей*** модели, требует огромного количества ресурсов, как по сбору и разметке датасета (например OpenAI достигло этого за счёт найма [низкооплачиваемых работников из Кении](https://time.com/6247678/openai-chatgpt-kenya-workers/) на аутсорс, которые вручную очищали датасет от оскорблений и нецензурной лексики), так и для обучения модели понадобятся огромные вычислительные мощности, множество GPU-часов.

Например, вариация модели LLaMa 2 имеющая 70 миллиардов параметров и контекстное окно = 4096 токенов, выпущенная в июле 2023 компанией Meta, [обучалась](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#hardware-and-software) на видеокартах `NVidia A100 80GB` в течении 1 720 320 GPU-часов - (если учесть что цена розничной аренды на подобные видеокарты сейчас около $1.9 / час - обучение подобной модели займёт не менее 3 миллионов долларов)

Т.е. затраты на создание LLM путём увеличения их размера, количества параметров - не могут быть покрыты энтузиастами-любителями или даже средними компаниями, соответственно это естественным образом создаёт разрыв в возможностях разработки LLM в пользу коммерческих гигантов таких как FANG-подобных (Facebook, Amazon, Netflix, Google), а в России это конечно Сбер и Яндекс.

Всё это, само собой, выгодно для крупных компаний, т.к. они могут создавать такие огромные модели, а дальше бесконечно продавать их как услугу по подписке, или же по модели *SaS* (Software-as-Service) вместе с инфраструктурой (например векторной базой данных, в которую будет добавляться обрабатываемая информация), а также будут дообучать модели прямо на ваших данных (как OpenAI и делает, о чём говорит открыто), используя эти данные различным [непредвиденным образом](https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak). Помимо этого, ваш бизнес будет таким образом завязан на сторонние сервисы, а сменить "провайдера нейросети" будет нелегко т.к. и техническая инфраструктура, да и сами данные будут прочно завязаны на определённого провайдера *услуг искусственного интеллекта*, что само по себе может быть и не является проблемой, но конечно подходит далеко не всем.

### Открытые и "открытые" большие языковые модели

Значит ли это что без использования коммерческих моделей вроде GPT-3.5, GPT-4 (для обработки естественного языка) или Midjourney (для генерации изображений) получить удовлетворительные результаты невозможно? - *Нет, не совсем так*

За последний год появилось множество открытых языковых моделей: какие-то [открыты полностью](https://github.com/eugeneyan/open-llms), некоторые использовать для коммерции запрещает лицензия...

Модели которые были выбраны нами для использования в проекте:

| Название                         | Лицензия           | Ссылка на модель                                                      | Задача              | Комментарий                                             |
|----------------------------------|--------------------|-----------------------------------------------------------------------|---------------------|---------------------------------------------------------|
| MBARTRuSumGazeta                 | Apache License 2.0 | [huggingface](https://huggingface.co/IlyaGusev/mbart_ru_sum_gazeta)   | summarization       | лучшая открытая модель из существующих для суммаризации |
| Saiga Mistral-7b                 |                    | [huggingface](https://huggingface.co/IlyaGusev/saiga_mistral_7b_lora) | conversational, RAG |                                                         |
| cointegrated/rut5-base-absum     |                    | [huggingface](https://huggingface.co/cointegrated/rut5-base-absum)    |                     |                                                         |
| ruT5 base summarization (gazeta) |                    | [huggingface](https://huggingface.co/IlyaGusev/rut5_base_sum_gazeta)  |                     |                                                         |

#### LoRA

- [репозиторий GitHub](https://github.com/microsoft/LoRA)

### Техники улучшения RAG

1. Self Querying Retrieval
2. Parent Document Retriever
3. Hypothetical Document Embeddings
4. Contextual Compressors & Filters
5. Hypothetical Document Embeddings
6. RAG Fusion
